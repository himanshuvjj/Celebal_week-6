##  Model Evaluation and Hyperparameter Tuning

###  Project Overview

This project demonstrates the evaluation and optimization of multiple machine learning models using the Iris dataset. It includes preprocessing, training, metric-based evaluation, and hyperparameter tuning using `GridSearchCV` and `RandomizedSearchCV` from Scikit-learn.

---

###  Contents

* `Model Evaluation and Hyperparameter Tuning.ipynb` — Jupyter notebook with the full pipeline.
* `README.md` — This documentation file.

---

###  Objectives

* Load and preprocess the dataset
* Train and evaluate multiple models (Logistic Regression, Random Forest, SVM)
* Perform hyperparameter tuning
* Compare models and identify the best performer

---

###  Tools & Libraries

* Python
* NumPy & Pandas
* Scikit-learn
* Jupyter Notebook

---

###  Techniques Used

* **Train-Test Split**
* **Feature Scaling**
* **Model Evaluation:**

  * Accuracy
  * Precision
  * Recall
  * F1 Score
* **Hyperparameter Tuning:**

  * `GridSearchCV` for Random Forest
  * `RandomizedSearchCV` for SVM

---

###  Results Summary

After tuning, the best model was selected based on F1-score performance. The workflow successfully improves classification accuracy through strategic parameter optimization.

---

###  Key Learnings

* Importance of evaluating models on multiple metrics
* How to apply grid and randomized search for parameter tuning
* How to standardize and structure an ML evaluation pipeline

---


